receivers:
  hostmetrics:
    collection_interval: 15s  # Balanced interval
    root_path: /hostfs
    scrapers:
      process:
        include:
          match_type: regexp
          names: [".*"]
        exclude:
          names: ["otelcol", "gopsutil_*", "\\[migration/.*\\]", "\\[rcu_.*\\]"]
        metrics:
          process.cpu.time:
            enabled: true
          process.cpu.utilization:
            enabled: true
          process.memory.physical:
            enabled: true
          process.memory.virtual:
            enabled: true
          process.disk.io:
            enabled: true
          process.threads:
            enabled: true
          process.open_file_descriptors:
            enabled: true

processors:
  memory_limiter:
    check_interval: 1s
    limit_percentage: 80
    spike_limit_percentage: 20
  
  cumulativetodelta:
    include:
      metrics:
        - process.cpu.time
        - process.disk.io
  
  resourcedetection/system:
    detectors: [env, system, ec2, gcp, azure]
    system:
      hostname_sources: ["os"]
    timeout: 2s
    override: false
  
  resource/add_experiment_info:
    attributes:
      - key: phoenix.experiment.id
        value: ${PHOENIX_EXPERIMENT_ID}
        action: insert
      - key: phoenix.variant
        value: ${PHOENIX_VARIANT}
        action: insert
      - key: node.name
        value: ${NODE_NAME}
        action: upsert
      - key: collection.strategy
        value: "intelligent"
        action: insert
  
  transform/intelligent_classification:
    metric_statements:
      - context: datapoint
        statements:
          # Comprehensive process classification
          - set(attributes["process.tier"], "tier1") 
            where attributes["process.executable.name"] matches "^(postgres|mysql|mongodb|redis|cassandra|elasticsearch|clickhouse)"
          - set(attributes["process.category"], "database")
            where attributes["process.executable.name"] matches "^(postgres|mysql|mongodb|redis|cassandra|elasticsearch|clickhouse)"
          
          - set(attributes["process.tier"], "tier1")
            where attributes["process.executable.name"] matches "^(nginx|apache|haproxy|envoy|traefik|istio)"
          - set(attributes["process.category"], "loadbalancer") 
            where attributes["process.executable.name"] matches "^(nginx|apache|haproxy|envoy|traefik|istio)"
          
          - set(attributes["process.tier"], "tier2")
            where attributes["process.executable.name"] matches "^(java|python|node|ruby|go|php|dotnet)"
          - set(attributes["process.category"], "application")
            where attributes["process.executable.name"] matches "^(java|python|node|ruby|go|php|dotnet)"
          
          - set(attributes["process.tier"], "tier1")
            where attributes["process.executable.name"] matches "^(kubelet|dockerd|containerd|crio|runc)"
          - set(attributes["process.category"], "container_runtime")
            where attributes["process.executable.name"] matches "^(kubelet|dockerd|containerd|crio|runc)"
          
          - set(attributes["process.tier"], "tier2")
            where attributes["process.executable.name"] matches "^(systemd|init|networkd|chronyd|sshd)"
          - set(attributes["process.category"], "system_service")
            where attributes["process.executable.name"] matches "^(systemd|init|networkd|chronyd|sshd)"
          
          - set(attributes["process.tier"], "tier3")
            where attributes["process.tier"] == nil
          - set(attributes["process.category"], "other")
            where attributes["process.category"] == nil
          
          # Performance-based classification
          - set(attributes["process.performance"], "high")
            where attributes["process.cpu.utilization"] > 10.0 or 
                  attributes["process.memory.physical"] > 500000000  # >500MB
          
          - set(attributes["process.performance"], "medium")
            where (attributes["process.cpu.utilization"] > 2.0 and attributes["process.cpu.utilization"] <= 10.0) or
                  (attributes["process.memory.physical"] > 100000000 and attributes["process.memory.physical"] <= 500000000)
          
          - set(attributes["process.performance"], "low")
            where attributes["process.performance"] == nil
          
          # Business criticality assessment
          - set(attributes["process.criticality"], "critical")
            where attributes["process.tier"] == "tier1"
          
          - set(attributes["process.criticality"], "important") 
            where attributes["process.tier"] == "tier2" and attributes["process.performance"] != "low"
          
          - set(attributes["process.criticality"], "normal")
            where attributes["process.criticality"] == nil
  
  # Intelligent filtering based on multiple dimensions
  filter/intelligent_keep:
    metrics:
      datapoint:
        # Always keep critical processes regardless of performance
        - 'attributes["process.criticality"] == "critical"'
        
        # Keep important processes with medium+ performance
        - 'attributes["process.criticality"] == "important" and attributes["process.performance"] != "low"'
        
        # Sample normal processes based on performance
        - 'attributes["process.criticality"] == "normal" and attributes["process.performance"] == "high"'
        - 'attributes["process.criticality"] == "normal" and attributes["process.performance"] == "medium" and random() < 0.3'
        - 'attributes["process.criticality"] == "normal" and attributes["process.performance"] == "low" and random() < 0.05'
  
  # Multi-level aggregation for different tiers
  groupbyattrs/tier1:
    keys:
      - host.name
      - process.executable.name
      - process.pid
      - process.category
      - phoenix.experiment.id
      - phoenix.variant
    # Keep full granularity for tier1 processes
  
  groupbyattrs/tier2_and_3:
    keys:
      - host.name
      - process.executable.name  # Aggregate by process name only
      - process.category
      - process.performance
      - phoenix.experiment.id
      - phoenix.variant
    # Aggregate tier2/3 processes by name and performance level
  
  batch:
    send_batch_size: 800
    timeout: 12s
    send_batch_max_size: 1600

exporters:
  otlphttp/newrelic:
    endpoint: ${NEW_RELIC_OTLP_ENDPOINT:-https://otlp.nr-data.net}
    headers:
      api-key: ${NEW_RELIC_API_KEY}
    compression: gzip
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 300s
    sending_queue:
      enabled: true
      num_consumers: 2
      queue_size: 800
  
  prometheus:
    endpoint: 0.0.0.0:8888
    namespace: phoenix
    const_labels:
      experiment_id: ${PHOENIX_EXPERIMENT_ID}
      variant: ${PHOENIX_VARIANT}
      strategy: intelligent
    resource_to_telemetry_conversion:
      enabled: true
    enable_open_metrics: true

extensions:
  health_check:
    endpoint: 0.0.0.0:13133
  
  pprof:
    endpoint: 0.0.0.0:1777
  
  zpages:
    endpoint: 0.0.0.0:55679

service:
  extensions: [health_check, pprof, zpages]
  pipelines:
    metrics:
      receivers: [hostmetrics]
      processors:
        - memory_limiter
        - cumulativetodelta
        - resourcedetection/system
        - resource/add_experiment_info
        - transform/intelligent_classification
        - filter/intelligent_keep
        - batch
      exporters: [otlphttp/newrelic, prometheus]
  
  telemetry:
    logs:
      level: info
      output_paths: ["stdout"]
      error_output_paths: ["stderr"]
    metrics:
      level: detailed
      address: 0.0.0.0:8889

# Template Metadata
metadata:
  name: "Process Intelligent Classification v1"
  description: "Advanced multi-dimensional process classification with performance-aware filtering"
  strategy: "intelligent"
  expected_reduction: "50-75%"
  use_cases:
    - "Production environments requiring balanced visibility and cost"
    - "Complex multi-tier applications"
    - "Environments with diverse workload patterns"
    - "SRE teams needing intelligent alerting"
  features:
    - "Multi-tier process classification (tier1/tier2/tier3)"
    - "Performance-based categorization (high/medium/low)"
    - "Business criticality assessment (critical/important/normal)"
    - "Multi-dimensional filtering logic"
    - "Adaptive sampling rates by importance"
    - "Granular aggregation strategies"
  classification_logic:
    tier1: "Critical infrastructure (databases, load balancers, container runtimes)"
    tier2: "Important services (applications, system services)"
    tier3: "Everything else"
  filtering_strategy:
    critical: "100% retention"
    important: "High/medium performance only"
    normal: "Sampled based on performance (high=100%, medium=30%, low=5%)"
  parameters:
    tier1_processes:
      description: "Process patterns for tier1 classification"
      default: "databases, load balancers, container runtimes"
      configurable: true
    performance_thresholds:
      cpu_high: "10%"
      cpu_medium: "2%"
      memory_high: "500MB"
      memory_medium: "100MB"
      configurable: true
    sampling_rates:
      normal_medium: "30%"
      normal_low: "5%"
      configurable: true