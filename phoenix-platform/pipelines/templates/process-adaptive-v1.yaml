receivers:
  hostmetrics:
    collection_interval: 30s  # Adaptive: longer interval during low activity
    root_path: /hostfs
    scrapers:
      process:
        include:
          match_type: regexp
          names: [".*"]
        exclude:
          names: ["otelcol", "gopsutil_*", "kernel.*", "\\[.*\\]"]  # Exclude kernel threads
        metrics:
          process.cpu.time:
            enabled: true
          process.cpu.utilization:
            enabled: true
          process.memory.physical:
            enabled: true
          process.memory.virtual:
            enabled: false  # Disable virtual memory for volume reduction
          process.disk.io:
            enabled: true
          process.threads:
            enabled: false  # Disable thread count for volume reduction
          process.open_file_descriptors:
            enabled: false  # Disable FD count for volume reduction

processors:
  memory_limiter:
    check_interval: 1s
    limit_percentage: 75  # More aggressive memory limiting
    spike_limit_percentage: 15
  
  cumulativetodelta:
    include:
      metrics:
        - process.cpu.time
        - process.disk.io
  
  resourcedetection/system:
    detectors: [env, system, ec2, gcp, azure]
    system:
      hostname_sources: ["os"]
    timeout: 2s
    override: false
  
  resource/add_experiment_info:
    attributes:
      - key: phoenix.experiment.id
        value: ${PHOENIX_EXPERIMENT_ID}
        action: insert
      - key: phoenix.variant
        value: ${PHOENIX_VARIANT}
        action: insert
      - key: node.name
        value: ${NODE_NAME}
        action: upsert
      - key: collection.strategy
        value: "adaptive"
        action: insert
  
  transform/adaptive_classification:
    metric_statements:
      - context: datapoint
        statements:
          # Mark processes as active/idle based on recent resource usage
          - set(attributes["process.activity"], "active") 
            where attributes["process.cpu.utilization"] > 1.0 or 
                  attributes["process.memory.physical"] > 50000000  # >50MB
          
          - set(attributes["process.activity"], "idle")
            where attributes["process.activity"] == nil
          
          # Classify by process type for different handling
          - set(attributes["process.type"], "database")
            where attributes["process.executable.name"] matches "^(postgres|mysql|mongodb|redis|cassandra|elasticsearch)"
          
          - set(attributes["process.type"], "webserver")
            where attributes["process.executable.name"] matches "^(nginx|apache|haproxy|envoy|traefik)"
          
          - set(attributes["process.type"], "runtime")
            where attributes["process.executable.name"] matches "^(python|java|node|ruby|go|php)"
          
          - set(attributes["process.type"], "system")
            where attributes["process.executable.name"] matches "^(systemd|init|kernel|kthread)"
          
          - set(attributes["process.type"], "container")
            where attributes["process.executable.name"] matches "^(docker|containerd|runc|podman)"
          
          - set(attributes["process.type"], "other")
            where attributes["process.type"] == nil
  
  # Adaptive filtering: more aggressive for idle processes
  filter/adaptive_keep:
    metrics:
      datapoint:
        # Always keep active databases and webservers
        - 'attributes["process.type"] == "database" or attributes["process.type"] == "webserver"'
        
        # Keep active runtime processes
        - 'attributes["process.type"] == "runtime" and attributes["process.activity"] == "active"'
        
        # Sample idle runtime processes at 10%
        - 'attributes["process.type"] == "runtime" and attributes["process.activity"] == "idle" and random() < 0.1'
        
        # Keep important system processes
        - 'attributes["process.type"] == "system" and attributes["process.executable.name"] matches "^(systemd|init)"'
        
        # Keep active container processes
        - 'attributes["process.type"] == "container" and attributes["process.activity"] == "active"'
        
        # Sample other processes at 5%
        - 'attributes["process.type"] == "other" and random() < 0.05'
  
  # Aggregate metrics by process type to reduce cardinality
  groupbyattrs/process_type:
    keys:
      - host.name
      - process.type
      - process.activity
      - phoenix.experiment.id
      - phoenix.variant
  
  batch:
    send_batch_size: 750
    timeout: 20s  # Longer timeout for adaptive collection
    send_batch_max_size: 1500

exporters:
  otlphttp/newrelic:
    endpoint: ${NEW_RELIC_OTLP_ENDPOINT:-https://otlp.nr-data.net}
    headers:
      api-key: ${NEW_RELIC_API_KEY}
    compression: gzip
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 300s
    sending_queue:
      enabled: true
      num_consumers: 2
      queue_size: 750
  
  prometheus:
    endpoint: 0.0.0.0:8888
    namespace: phoenix
    const_labels:
      experiment_id: ${PHOENIX_EXPERIMENT_ID}
      variant: ${PHOENIX_VARIANT}
      strategy: adaptive
    resource_to_telemetry_conversion:
      enabled: true
    enable_open_metrics: true

extensions:
  health_check:
    endpoint: 0.0.0.0:13133
  
  pprof:
    endpoint: 0.0.0.0:1777
  
  zpages:
    endpoint: 0.0.0.0:55679

service:
  extensions: [health_check, pprof, zpages]
  pipelines:
    metrics:
      receivers: [hostmetrics]
      processors:
        - memory_limiter
        - cumulativetodelta
        - resourcedetection/system
        - resource/add_experiment_info
        - transform/adaptive_classification
        - filter/adaptive_keep
        - groupbyattrs/process_type
        - batch
      exporters: [otlphttp/newrelic, prometheus]
  
  telemetry:
    logs:
      level: info
      output_paths: ["stdout"]
      error_output_paths: ["stderr"]
    metrics:
      level: detailed
      address: 0.0.0.0:8889

# Template Metadata
metadata:
  name: "Process Adaptive Filter v1"
  description: "Dynamically adjusts collection based on process activity and type, with intelligent sampling"
  strategy: "adaptive"
  expected_reduction: "70-90%"
  use_cases:
    - "Dynamic environments with varying load patterns"
    - "Cost-sensitive deployments requiring maximum reduction"
    - "Environments with predictable quiet periods"
  features:
    - "Activity-based classification (active/idle)"
    - "Process type-aware filtering"
    - "Adaptive sampling rates"
    - "Metric aggregation by process type"
    - "Reduced metric dimensions"
  parameters:
    collection_interval:
      description: "Base collection interval"
      default: "30s"
      configurable: true
    idle_sampling_rate:
      description: "Sampling rate for idle processes"
      default: "10%"
      configurable: true
    memory_threshold:
      description: "Memory threshold for active classification"
      default: "50MB"
      configurable: true