receivers:
  hostmetrics:
    collection_interval: 30s
    root_path: /host/proc
    scrapers:
      cpu: {}
      memory: {}
      process:
        mute_process_name_error: true
      filesystem: {}
      disk: {}
      load: {}
  
  # Enhanced control signal receiver
  hostmetrics/control:
    collection_interval: 10s
    scrapers:
      load: {} # Minimal scraper just to have something
    
  # File-based control signal reader for optimization mode
  filelog/control_signals:
    include: [/etc/otelcol/control_signals/opt_mode.yaml]
    start_at: beginning
    include_file_path: true
    operators:
      - type: regex_parser
        regex: '.*mode: (?P<mode>\w+).*'
        parse_from: body

processors:
  # Memory limiter - protects the collector from memory spikes
  memory_limiter:
    check_interval: 5s
    limit_percentage: 80
    spike_limit_percentage: 90

  # General batching
  batch:
    send_batch_size: 1000
    timeout: 10s
    
  # Retry logic for resilience
  retry:
    enabled: true
    initial_interval: 5s
    max_interval: 30s
    max_elapsed_time: 5m
    
  # Enhanced resource detection
  resourcedetection:
    detectors: [env, system, gcp, aws, azure, docker] # Add cloud provider detectors
    timeout: 30s
    system:
      hostname_sources: [os, dns, provider]
  
  # Add custom attributes to all metrics for better context
  attributes:
    actions:
      - key: service.name
        value: "phoenix-observability"
        action: insert
      - key: service.version
        value: "1.0.0"
        action: insert
      - key: deployment.environment
        value: ${DEPLOYMENT_ENV:-development}
        action: insert
  
  # Pipeline-specific processors
  filter/full_pipeline:
    # Full pipeline passes everything through - no filtering
    metrics:
      include: {}
      
  # Dynamic filter control based on control file mode
  filter/opt_pipeline:
    # Optimized pipeline with enhanced filtering - balance between data reduction and observability
    metrics:
      include:
        match_type: regexp
        metric_names:
          # CPU metrics - essential for performance monitoring
          - "system.cpu.utilization"
          - "system.cpu.load_average.*"
          - "system.cpu.time"
          
          # Memory metrics - critical for resource monitoring
          - "system.memory.usage"
          - "system.memory.utilization"
          
          # Disk metrics - critical for storage monitoring
          - "system.filesystem.utilization"
          - "system.disk.io"
          - "system.disk.operations"
          
          # Process metrics - only the most critical ones
          - "process.cpu.time"
          - "process.memory.usage"
          - "process.uptime"
          
          # Critical status metrics
          - ".*error.*"
          - ".*failure.*"
          - ".*status.*"
          
          # Custom app metrics
          - "phoenix.observability.*"

  filter/exp_pipeline:
    # Experimental pipeline with enhanced space-saving techniques
    metrics:
      include:
        match_type: regexp
        metric_names:
          # System metrics - critical for observability
          - "system.cpu.*"
          - "system.memory.*"
          - "system.filesystem.utilization"
          - "system.disk.io"
          - "system.network.*"
          
          # Process metrics - only the essential ones
          - "process.cpu.time"
          - "process.memory.usage"
          - "process.uptime"
          
          # Error metrics - always preserve
          - ".*error.*"
          - ".*failure.*"
          
          # Custom application metrics
          - "phoenix.observability.*"
          
  # Hybrid pipeline filter combining optimized and experimental approaches
  filter/hybrid_pipeline:
    # Balanced approach for filtering - selective but comprehensive
    metrics:
      include:
        match_type: regexp
        metric_names:
          # System metrics with selective approach
          - "system.cpu.utilization"
          - "system.cpu.time"
          - "system.cpu.load_average.1m"
          - "system.memory.usage"
          - "system.memory.utilization"
          - "system.filesystem.utilization"
          - "system.disk.io"
          - "system.network.(dropped|errors|io)"
          
          # Process metrics - balance between detail and volume
          - "process.cpu.time"
          - "process.memory.usage"
          - "process.uptime"
          
          # All error and status metrics - comprehensive diagnostics
          - ".*error.*"
          - ".*failure.*"
          - ".*status.*"
          
          # Custom and application metrics - preserve important ones
          - "phoenix.observability.*"

  # Add pipeline labels
  metricstransform/full:
    transforms:
      - include: ".*"
        match_type: regexp
        action: update
        operations:
          - action: add_label
            new_label: pipeline
            new_value: "full"
  
  # Control signal processor to read the optimization mode
  transform/control_reader:
    metric_statements:
      - context: datapoint
        statements:
          - set(resource.attributes["optimization_mode"], attributes["optimization_mode"]) where attributes["optimization_mode"] != nil

  # Enhanced transformations for the optimized pipeline
  metricstransform/opt:
    transforms:
      # Base pipeline labeling
      - include: ".*"
        match_type: regexp
        action: update
        operations:
          - action: add_label
            new_label: pipeline
            new_value: "optimized"
          - action: add_label
            new_label: optimization_level
            new_value: "{{ if eq .optimization_mode \"ultra\" }}ultra{{ else }}moderate{{ end }}"
          
      # Aggregations for moderate mode - balance between detail and volume
      - include: "system.cpu.time"
        match_type: regexp
        action: update
        operations:
          - action: aggregate_labels
            label_set: ["cpu", "state", "pipeline"]
            aggregation_type: sum
      
      # Additional aggregations for ultra mode - maximum cardinality reduction
      - include: "system.memory.usage"
        match_type: regexp
        action: update
        operations:
          - action: aggregate_labels
            label_set: ["state", "pipeline"]
            aggregation_type: sum
      
  # Space-saving processor for experimental pipeline  
  transform/space_saving:
    # Advanced cardinality control with space-saving algorithm logic
    metric_statements:
      # Implement top-k algorithm for high-cardinality metrics
      - context: resource
        statements:
          # Set a reasonable limit for high-cardinality dimensions
          - set(resource.attributes["exp.topk.limit"], 20)
          # Mark metrics for dimensionality reduction
          - set(resource.attributes["exp.dimensionality.reduction"], true) 
      
      # Keep only the most significant dimensions for process metrics
      - context: metric
        statements:
          - keep_keys(attributes, ["process.pid", "process.executable.name"]) where metric.name == "process.cpu.time"
          - keep_keys(attributes, ["process.pid", "process.executable.name"]) where metric.name == "process.memory.usage"
      
      # Keep only the most significant dimensions for system metrics
      - context: metric
        statements:
          - keep_keys(attributes, ["cpu", "state"]) where metric.name == "system.cpu.time"
          - keep_keys(attributes, ["state"]) where metric.name == "system.memory.usage"
          - keep_keys(attributes, ["device", "direction"]) where metric.name == "system.network.io"
          - keep_keys(attributes, ["device", "direction"]) where metric.name == "system.disk.io"

  # Histogram bucketing and value processing
  metricstransform/exp:
    transforms:
      # Base pipeline labeling
      - include: ".*"
        match_type: regexp
        action: update
        operations:
          - action: add_label
            new_label: pipeline
            new_value: "experimental"
          - action: add_label
            new_label: algorithm
            new_value: "space-saving"
      
      # Implement top-K keeping only the highest-value metrics
      - include: "system\\..*"
        match_type: regexp
        action: update
        operations:
          # Implement label aggregation with sum
          - action: aggregate_labels
            label_set: ["pipeline", "algorithm"] # Only keep pipeline and algorithm labels
            aggregation_type: sum
      
      # Reduced cardinality for process metrics  
      - include: "process\\..*"
        match_type: regexp
        action: update
        operations:
          # Group by executable name instead of individual PIDs
          - action: aggregate_labels
            label_set: ["process.executable.name", "pipeline", "algorithm"]
            aggregation_type: sum
            
  # Hybrid approach transformation
  transform/hybrid:
    # Selective attribute preservation with intelligent dimensionality
    metric_statements:
      # Mark metrics as hybrid approach
      - context: resource
        statements:
          - set(resource.attributes["hybrid.approach"], true)
      
      # Selective dimensionality reduction for CPU metrics
      - context: metric
        statements:
          # Preserve most important dimensions for CPU metrics
          - keep_keys(attributes, ["cpu", "state", "core"]) where metric.name == "system.cpu.time"
          - keep_keys(attributes, ["state"]) where metric.name == "system.memory.usage"
          # Keep important context for process metrics
          - keep_keys(attributes, ["process.executable.name", "process.command"]) where metric.name == "process.cpu.time"
  
  # Hybrid pipeline transformations
  metricstransform/hybrid:
    transforms:
      # Base pipeline labeling
      - include: ".*"
        match_type: regexp
        action: update
        operations:
          - action: add_label
            new_label: pipeline
            new_value: "hybrid"
          - action: add_label
            new_label: approach
            new_value: "balanced"
      
      # Selective aggregations for system metrics
      - include: "system.cpu.time"
        match_type: regexp
        action: update
        operations:
          # Maintain some detail but reduce cardinality
          - action: aggregate_labels
            label_set: ["cpu", "state", "pipeline"] 
            aggregation_type: sum
      
      # Moderate dimensionality reduction for memory metrics
      - include: "system.memory.usage"
        match_type: regexp
        action: update
        operations:
          - action: aggregate_labels
            label_set: ["state", "pipeline"]
            aggregation_type: sum
            
      # Process-level aggregation that preserves executable info
      - include: "process.cpu.time"
        match_type: regexp
        action: update
        operations:
          - action: aggregate_labels
            label_set: ["process.executable.name", "pipeline"]
            aggregation_type: sum

exporters:
  # Prometheus exporter for local visualization
  prometheus:
    endpoint: 0.0.0.0:8888
    namespace: phoenix
    send_timestamps: true
    metric_expiration: 180m
    resource_to_telemetry_conversion: 
      enabled: true
    const_labels:
      collector_version: "0.103.0" 
      environment: "${DEPLOYMENT_ENV:-development}"
    
  # New Relic exporters for each pipeline
  otlp/newrelic_full:
    endpoint: "https://otlp.nr-data.net:4317"
    headers:
      api-key: ${NR_FULL_KEY}
    timeout: 20s
    sending_queue:
      enabled: true
      num_consumers: 4
      queue_size: 10000
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 5m
    
  otlp/newrelic_opt:
    endpoint: "https://otlp.nr-data.net:4317"
    headers:
      api-key: ${NR_OPT_KEY}
    timeout: 20s
    sending_queue:
      enabled: true
      num_consumers: 4
      queue_size: 10000
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 5m
    
  otlp/newrelic_exp:
    endpoint: "https://otlp.nr-data.net:4317" 
    headers:
      api-key: ${NR_EXP_KEY}
    timeout: 20s
    sending_queue:
      enabled: true
      num_consumers: 4
      queue_size: 10000
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 5m
      
  otlp/newrelic_hybrid:
    endpoint: "https://otlp.nr-data.net:4317" 
    headers:
      api-key: ${NR_HYBRID_KEY:-${NR_EXP_KEY}}
    timeout: 20s
    sending_queue:
      enabled: true
      num_consumers: 4
      queue_size: 10000
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 5m

extensions:
  health_check:
    endpoint: 0.0.0.0:13133
    
  pprof:
    endpoint: 0.0.0.0:1777
    
  zpages:
    endpoint: 0.0.0.0:55679

service:
  extensions: [health_check, pprof, zpages]
  
  telemetry:
    metrics:
      address: 0.0.0.0:8889
      level: detailed
    logs:
      level: info
      development: false
      encoding: json
      initial_fields:
        service: "phoenix-observability"
        version: "1.0.0"

  pipelines:
    # Full pipeline - minimal processing (truth source) with enhanced resilience
    metrics/full:
      receivers: [hostmetrics]
      processors: [memory_limiter, resourcedetection, attributes, filter/full_pipeline, metricstransform/full, batch, retry]
      exporters: [prometheus, otlp/newrelic_full]
      
    # Optimized pipeline - dynamic filtering based on time series count
    metrics/optimized:
      receivers: [hostmetrics, hostmetrics/control, filelog/control_signals]
      processors: [memory_limiter, resourcedetection, attributes, filter/opt_pipeline, transform/control_reader, metricstransform/opt, batch, retry]
      exporters: [prometheus, otlp/newrelic_opt]
      
    # Experimental pipeline - advanced space-saving techniques
    metrics/experimental:
      receivers: [hostmetrics]
      processors: [memory_limiter, resourcedetection, attributes, filter/exp_pipeline, transform/space_saving, metricstransform/exp, batch, retry]
      exporters: [prometheus, otlp/newrelic_exp]
      
    # Hybrid pipeline - combined approach for optimal balance
    metrics/hybrid:
      receivers: [hostmetrics]
      processors: [memory_limiter, resourcedetection, attributes, filter/hybrid_pipeline, transform/hybrid, metricstransform/hybrid, batch, retry]
      exporters: [prometheus, otlp/newrelic_hybrid]