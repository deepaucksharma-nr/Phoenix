receivers:
  hostmetrics:
    collection_interval: 60s  # Very long interval for minimal data
    root_path: /hostfs
    scrapers:
      process:
        include:
          match_type: regexp
          # Only monitor specific critical processes
          names: 
            - "^(postgres|mysql|redis|mongodb|elasticsearch)"
            - "^(nginx|apache|haproxy|envoy)"
            - "^(java|python|node).*"
            - "^(docker|containerd|kubelet)"
        exclude:
          names: ["otelcol", "gopsutil_*"]
        metrics:
          process.cpu.time:
            enabled: false  # Disable cumulative metrics to reduce volume
          process.cpu.utilization:
            enabled: true   # Only keep utilization (more useful)
          process.memory.physical:
            enabled: true   # Essential memory metric
          process.memory.virtual:
            enabled: false  # Disable virtual memory
          process.disk.io:
            enabled: false  # Disable disk I/O for minimal setup
          process.threads:
            enabled: false  # Disable thread count
          process.open_file_descriptors:
            enabled: false  # Disable file descriptors

processors:
  memory_limiter:
    check_interval: 2s
    limit_percentage: 70
    spike_limit_percentage: 10
  
  resourcedetection/system:
    detectors: [env, system]  # Only basic detectors
    system:
      hostname_sources: ["os"]
    timeout: 2s
    override: false
  
  resource/add_experiment_info:
    attributes:
      - key: phoenix.experiment.id
        value: ${PHOENIX_EXPERIMENT_ID}
        action: insert
      - key: phoenix.variant
        value: ${PHOENIX_VARIANT}
        action: insert
      - key: node.name
        value: ${NODE_NAME}
        action: upsert
      - key: collection.strategy
        value: "minimal"
        action: insert
  
  transform/essential_only:
    metric_statements:
      - context: datapoint
        statements:
          # Only keep metrics for processes using significant resources
          - drop()
            where name == "process.cpu.utilization" and 
                  attributes["process.cpu.utilization"] < 2.0
          
          - drop()
            where name == "process.memory.physical" and 
                  attributes["process.memory.physical"] < 20000000  # < 20MB
  
  # Heavy aggregation to minimize cardinality
  groupbyattrs/minimal:
    keys:
      - host.name
      - process.executable.name  # Group by process name only
      - phoenix.experiment.id
      - phoenix.variant
  
  batch:
    send_batch_size: 100   # Very small batches
    timeout: 60s           # Long timeout
    send_batch_max_size: 200

exporters:
  otlphttp/newrelic:
    endpoint: ${NEW_RELIC_OTLP_ENDPOINT:-https://otlp.nr-data.net}
    headers:
      api-key: ${NEW_RELIC_API_KEY}
    compression: gzip
    retry_on_failure:
      enabled: true
      initial_interval: 10s
      max_interval: 60s
      max_elapsed_time: 300s
    sending_queue:
      enabled: true
      num_consumers: 1  # Single consumer for minimal setup
      queue_size: 100
  
  prometheus:
    endpoint: 0.0.0.0:8888
    namespace: phoenix
    const_labels:
      experiment_id: ${PHOENIX_EXPERIMENT_ID}
      variant: ${PHOENIX_VARIANT}
      strategy: minimal
    resource_to_telemetry_conversion:
      enabled: true
    enable_open_metrics: false  # Disable OpenMetrics for less overhead

extensions:
  health_check:
    endpoint: 0.0.0.0:13133

service:
  extensions: [health_check]  # Minimal extensions
  pipelines:
    metrics:
      receivers: [hostmetrics]
      processors:
        - memory_limiter
        - resourcedetection/system
        - resource/add_experiment_info
        - transform/essential_only
        - groupbyattrs/minimal
        - batch
      exporters: [otlphttp/newrelic, prometheus]
  
  telemetry:
    logs:
      level: warn  # Minimal logging
      output_paths: ["stdout"]
    metrics:
      level: basic
      address: 0.0.0.0:8889

# Template Metadata
metadata:
  name: "Process Minimal Collection v1"
  description: "Absolute minimum process metrics collection for cost-critical environments"
  strategy: "minimal"
  expected_reduction: "95%+"
  use_cases:
    - "Cost-critical environments requiring maximum savings"
    - "POC/development environments"
    - "Compliance-only monitoring requirements"
    - "Emergency cost reduction scenarios"
  features:
    - "Only critical process types monitored"
    - "Minimal metric dimensions (CPU utilization, memory only)"
    - "Heavy aggregation by process name"
    - "Long collection intervals (60s)"
    - "Resource-based filtering (drop low-usage processes)"
    - "Minimal infrastructure overhead"
  parameters:
    collection_interval:
      description: "How often to collect metrics"
      default: "60s"
      configurable: true
    cpu_threshold:
      description: "Minimum CPU% to include in metrics"
      default: "2.0%"
      configurable: true
    memory_threshold:
      description: "Minimum memory usage to include"
      default: "20MB"
      configurable: true
  warnings:
    - "This template provides minimal visibility and should only be used when cost reduction is the primary concern"
    - "May miss important but low-resource processes"
    - "Not suitable for detailed performance analysis"